{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIKHxjpikhVA",
        "outputId": "d76c464d-9a32-4677-a59e-cc0c66a74c41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import os\n",
        "import string\n",
        "import logging\n",
        "import re  # Import regular expressions library\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3xbz4fkiwY",
        "outputId": "10852894-dcd3-4dbf-b78b-a9856623939b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "LEMMATIZER = WordNetLemmatizer ()"
      ],
      "metadata": {
        "id": "ONj_h1rlk4sQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_files(folder_path):\n",
        "    \"\"\"Reads all files in a folder and returns a dictionary\n",
        "    with filenames as keys and content as values.\"\"\"\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data[doc_id] = file.read()\n",
        "                doc_id_to_filename[doc_id] = filename  # Map doc_id to filename\n",
        "                logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "            doc_id += 1  # Increment document ID for the next file\n",
        "\n",
        "    return data, doc_id_to_filename"
      ],
      "metadata": {
        "id": "7Tqh-j0Vk5oW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Performs text cleaning: removing special characters, tokenization, stopword removal, and lemmatization.\"\"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and punctuation using regular expressions\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Keeps only alphanumeric characters and spaces\n",
        "\n",
        "    # Tokenize the cleaned text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
        "\n",
        "    return cleaned_tokens\n"
      ],
      "metadata": {
        "id": "znU3YoPCk5mH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverted_index(data):\n",
        "    \"\"\"Builds an inverted index from the cleaned text data and tracks term frequencies.\"\"\"\n",
        "\n",
        "    inverted_index = defaultdict(set)\n",
        "    term_frequencies = Counter()  # Track the frequency of each term\n",
        "\n",
        "    for doc_id, content in data.items():\n",
        "        cleaned_tokens = clean_text(content)\n",
        "\n",
        "        for token in cleaned_tokens:\n",
        "            inverted_index[token].add(doc_id)\n",
        "            term_frequencies[token] += 1  # Update term frequency\n",
        "\n",
        "    return inverted_index, term_frequencies"
      ],
      "metadata": {
        "id": "aspjXDEwk5jf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_query(query, inverted_index, doc_id_to_filename):\n",
        "    \"\"\"Processes a Boolean query ('AND', 'OR', 'NOT') on the inverted index and returns filenames.\"\"\"\n",
        "\n",
        "    query = query.lower()\n",
        "    tokens = query.split()\n",
        "\n",
        "    result_set = set()\n",
        "\n",
        "    if 'and' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "\n",
        "        # Check if all terms exist in the inverted index\n",
        "        if all(term in inverted_index for term in terms):\n",
        "            result_set = inverted_index[terms[0]]\n",
        "            for term in terms[1:]:\n",
        "                result_set = result_set.intersection(inverted_index[term])\n",
        "        else:\n",
        "            result_set = set()  # Return empty result if any term is missing\n",
        "\n",
        "    elif 'or' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        for term in terms:\n",
        "            if term in inverted_index:\n",
        "                if not result_set:\n",
        "                    result_set = inverted_index[term]\n",
        "                else:\n",
        "                    result_set = result_set.union(inverted_index[term])\n",
        "\n",
        "    elif 'not' in tokens:\n",
        "        term = tokens[1]\n",
        "        if term in inverted_index:\n",
        "            result_set = set(inverted_index.keys()) - inverted_index[term]\n",
        "        else:\n",
        "            result_set = set(inverted_index.keys())  # If term doesn't exist, return all docs\n",
        "\n",
        "    else:\n",
        "        # If no 'AND', 'OR', 'NOT' operators, check if single query term exists\n",
        "        if query in inverted_index:\n",
        "            result_set = inverted_index[query]\n",
        "        else:\n",
        "            result_set = set()  # If query term doesn't exist, return empty set\n",
        "\n",
        "    # Convert doc_ids to filenames\n",
        "    result_filenames = [doc_id_to_filename[doc_id] for doc_id in result_set if doc_id in doc_id_to_filename]\n",
        "\n",
        "    logging.info(f\"Query '{query}' resulted in: {result_filenames}\")\n",
        "\n",
        "    return result_filenames"
      ],
      "metadata": {
        "id": "6c9XBy3wl4hA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_queries_file(term_frequencies):\n",
        "    \"\"\"\n",
        "    Generates a queries.txt file with a variety of example queries based on the most frequent terms.\n",
        "    \"\"\"\n",
        "\n",
        "    # Open the file for writing\n",
        "    with open(\"queries.txt\", \"w\") as file:\n",
        "        # Get top 10 most common terms\n",
        "        most_common_terms = [term for term, freq in term_frequencies.most_common(10)]\n",
        "\n",
        "        # Write multiple 'AND' queries\n",
        "        if len(most_common_terms) >= 4:\n",
        "            and_query1 = f\"{most_common_terms[0]} AND {most_common_terms[1]}\"\n",
        "            and_query2 = f\"{most_common_terms[2]} AND {most_common_terms[3]}\"\n",
        "            file.write(f\"{and_query1}\\n\")\n",
        "            file.write(f\"{and_query2}\\n\")\n",
        "\n",
        "        # Write multiple 'OR' queries\n",
        "        if len(most_common_terms) >= 6:\n",
        "            or_query1 = f\"{most_common_terms[1]} OR {most_common_terms[2]}\"\n",
        "            or_query2 = f\"{most_common_terms[4]} OR {most_common_terms[5]}\"\n",
        "            file.write(f\"{or_query1}\\n\")\n",
        "            file.write(f\"{or_query2}\\n\")\n",
        "\n",
        "        # Write multiple 'NOT' queries\n",
        "        if len(most_common_terms) >= 6:\n",
        "            not_query1 = f\"NOT {most_common_terms[3]}\"\n",
        "            not_query2 = f\"NOT {most_common_terms[5]}\"\n",
        "            file.write(f\"{not_query1}\\n\")\n",
        "            file.write(f\"{not_query2}\\n\")\n",
        "\n",
        "        # Add some complex 'AND OR NOT' queries\n",
        "        if len(most_common_terms) >= 6:\n",
        "            complex_query1 = f\"{most_common_terms[0]} AND {most_common_terms[1]} OR NOT {most_common_terms[2]}\"\n",
        "            complex_query2 = f\"{most_common_terms[3]} OR {most_common_terms[4]} AND NOT {most_common_terms[5]}\"\n",
        "            file.write(f\"{complex_query1}\\n\")\n",
        "            file.write(f\"{complex_query2}\\n\")\n"
      ],
      "metadata": {
        "id": "ymun30ynpX2M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load dataset\n",
        "    folder_path = '/content/drive/MyDrive/dataset/docs'\n",
        "    data, doc_id_to_filename = load_text_files(folder_path)\n",
        "\n",
        "    # Build the inverted index and term frequencies\n",
        "    inverted_index, term_frequencies = build_inverted_index(data)\n",
        "\n",
        "    # Generate the queries.txt file\n",
        "    generate_queries_file(term_frequencies)\n",
        "\n",
        "    # Read the queries from the generated queries.txt file\n",
        "    with open(\"queries.txt\", \"r\") as query_file:\n",
        "        queries = query_file.readlines()\n",
        "\n",
        "    # Open a file to write the results\n",
        "    with open(\"query_results.txt\", \"w\") as result_file:\n",
        "        for query in queries:\n",
        "            query = query.strip()  # Remove any leading/trailing whitespace\n",
        "            if query:  # Skip empty lines\n",
        "                result = boolean_query(query, inverted_index, doc_id_to_filename)\n",
        "                result_str = f\"Results for '{query}': {result}\\n\"\n",
        "                print(result_str)  # Print to console\n",
        "                result_file.write(result_str)  # Write to file\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX-qc8Nfl4d6",
        "outputId": "2e24fff8-968e-40d6-b838-99f7989c8c05"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for 'dod AND bike': ['doc-6.txt', 'doc-4.txt', 'doc-10.txt', 'doc-8.txt', 'doc-1.txt', 'doc-5.txt']\n",
            "\n",
            "Results for 'ride AND one': ['doc-6.txt', 'doc-4.txt', 'doc-7.txt', 'doc-1.txt', 'doc-5.txt', 'doc-3.txt']\n",
            "\n",
            "Results for 'bike OR ride': ['doc-6.txt', 'doc-4.txt', 'doc-7.txt', 'doc-10.txt', 'doc-8.txt', 'doc-1.txt', 'doc-5.txt', 'doc-3.txt', 'doc-2.txt']\n",
            "\n",
            "Results for 'motorcycle OR like': ['doc-6.txt', 'doc-4.txt', 'doc-9.txt', 'doc-10.txt', 'doc-8.txt', 'doc-1.txt', 'doc-5.txt', 'doc-3.txt', 'doc-2.txt']\n",
            "\n",
            "Results for 'NOT one': []\n",
            "\n",
            "Results for 'NOT like': []\n",
            "\n",
            "Results for 'dod AND bike OR NOT ride': ['doc-6.txt', 'doc-4.txt', 'doc-10.txt', 'doc-1.txt', 'doc-5.txt']\n",
            "\n",
            "Results for 'one OR motorcycle AND NOT like': ['doc-6.txt', 'doc-4.txt', 'doc-1.txt']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kobb8M6Dl4bn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K4DoNCeDl4ZP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UrY4lUG5l4W3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-M1QWqhl4Ug"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StPC6_I1l4SF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XqnNrYxl4Pu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wL5YN7CAk5hW"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}